{"cells":[{"cell_type":"markdown","source":["# ML Pipelines with Spark ML\n\nThis example covers the concepts of Estimator, Transformer, and Param.\n\n\nPipelines API concept is mostly inspired by the scikit-learn project.\n\nhttps://spark.apache.org/docs/latest/ml-pipeline.html"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.classification import LogisticRegression"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## keywords\n\n* Vectors  : A vector has magnitude (size) and direction. Vectors are the metrics. A local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine.\n\n* MLlib supports two types of local vectors: dense and sparse. A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values. Being sparse means that it won’t explicitly contains each coordinate.\n\nEx. \n\n-a vector (1.0, 0.0, 3.0) can be represented in dense format as [1.0, 0.0, 3.0] or in sparse format as (3, [0, 2], [1.0, 3.0]), where 3 is the size of the vector.\n\n-dense vector (1, 2, 0, 0, 5, 0, 9, 0, 0) will be represented as sparse like {(0,1,4,6), (1, 2, 5, 9)}\n\n<br>\n\n* Transformers : converting datafeame into another dataframe\n\n-A feature transformer might take a DataFrame, read a column (e.g., text), map it into a new column (e.g., feature vectors), and output a new DataFrame with the mapped column appended.\n-A learning model might take a DataFrame, read the column containing feature vectors, predict the label for each feature vector, and output a new DataFrame with predicted labels appended as a column.\n\n<br>\n\n* Estimators: concept of a learning algorithm or any algorithm that fits or trains on data.  Technically, an Estimator implements a method fit(), which accepts a DataFrame and produces a Model, which is a Transformer. \n\n-For example, a learning algorithm such as LogisticRegression is an Estimator, and calling fit() trains a LogisticRegressionModel, which is a Model and hence a Transformer.\n\n<br>\n\n* Pipeline\n\nIn the Machine Learning Process data runs through different Stages, e.g. StringIndexer, VectorAssembler, VectorIndexer and RandomForestClassifier. These Stages can be combined to one workflow with a Pipeline. The DataFrame as the central data structure will be enriched at each Stage of the Pipeline.\n\n<br>\n\n* There are 2 types of Stages:\n\n-Estimator creates a model by calling the fit() method, e.g. StringIndexer, VectorIndexer and RandomForestClassifier\n\n-Transformer is a model and transforms a DataFrame by usually adding columns to the Input-DataFrame"],"metadata":{}},{"cell_type":"code","source":["# Prepare training data from a list of (label, features) tuples.\ndf_training = spark.createDataFrame([\n    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["# features and lables\n\nBriefly, feature is input; label is output.\n\nA feature is one column of the data in your input set. For instance, if you're trying to predict the type of pet someone will choose, your input features might include age, home region, family income, etc. The label is the final choice, such as dog, fish, iguana, rock, etc."],"metadata":{}},{"cell_type":"code","source":["#the training dataset\ndf_training.show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Create a LogisticRegression instance. This instance is an Estimator.\nlr = LogisticRegression(maxIter=10, regParam=0.01)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Logistic Regression\n\n\n* is widely used to predict a binary response.\n* is used to ascertain the probability of an event. And this event is captured in binary format, i.e. 0 or 1.\n* Example - I want to ascertain if a customer will buy my product or not. For this, I would run a Logistic Regression on the (relevant) data and my dependent variable would be a binary variable (1=Yes; 0=No).\n* that is, where the output can take only two values, \"0\" and \"1\", which represent outcomes such as pass/fail, win/lose, alive/dead or healthy/sick.\n* maxIter -  The number of iterations.\n* regParam - The regularizer parameter\n\n\n## Linear Regression\n\n * is used to establish a relationship between Dependent and Independent variables, which is useful in estimating the resultant dependent variable in case independent variable change.\n\n\n<br>\n\n* In terms of graphical representation, Linear Regression gives a linear line as an output, once the values are plotted on the graph. Whereas, the logistic regression gives an S-shaped line"],"metadata":{}},{"cell_type":"code","source":["# Print out the parameters, documentation, and any default values.\nprint(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["- explainParams() - Returns the documentation of all params with their optionally default values and user-supplied values."],"metadata":{}},{"cell_type":"code","source":["# Learn a LogisticRegression model. This uses the parameters stored in lr.\nmodel1 = lr.fit(df_training)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["* fit(dataset, params=None) \n\nFits a model to the input dataset with optional parameters.\n\nReturns the fitted model(s)"],"metadata":{}},{"cell_type":"code","source":["model1.write().overwrite().save(\"/FileStore/tables/modle1\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["The result of save for pipeline model is a JSON file for metadata while Parquet for model data, e.g. coefficients."],"metadata":{}},{"cell_type":"code","source":["%fs ls /FileStore/tables/modle1/data"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%fs head FileStore/tables/modle1/data/part-00000-tid-6344610780192366248-71efd7d1-6830-4c3c-8f6a-bacc284fff6f-144-c000.snappy.parquet"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%fs ls /FileStore/tables/modle1/metadata"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%fs head /FileStore/tables/modle1/metadata/part-00000 "],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Since model1 is a Model (i.e., a transformer produced by an Estimator),\n# we can view the parameters it used during fit().\n# This prints the parameter (name: value) pairs, where names are unique IDs for this\n# LogisticRegression instance.\nprint(\"Model 1 was fit using parameters: \")\nprint(model1.extractParamMap())"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["* extractParamMap\n\nExtracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra.\n\nReturns the\tmerged param map"],"metadata":{}},{"cell_type":"code","source":["# We may alternatively specify parameters using a Python dictionary as a paramMap\nparamMap = {lr.maxIter: 20}\nparamMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\nparamMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # Specify multiple Params."],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# You can combine paramMaps, which are python dictionaries.\nparamMap2 = {lr.probabilityCol: \"myProbability\"}  # Change output column name\nparamMapCombined = paramMap.copy()\nparamMapCombined.update(paramMap2)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# Now learn a new model using the paramMapCombined parameters.\n# paramMapCombined overrides all parameters set earlier via lr.set* methods.\nmodel2 = lr.fit(df_training, paramMapCombined)\nprint(\"Model 2 was fit using parameters: \")\nprint(model2.extractParamMap())"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# the input dataset\n# Prepare test data\ndf_test = spark.createDataFrame([\n    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Make predictions on test data using the Transformer.transform() method.\n# LogisticRegression.transform will only use the 'features' column.\n# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n# 'probability' column since we renamed the lr.probabilityCol parameter previously.\nprediction = model2.transform(df_test)\nresult = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\") \\\n    .collect()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["* transform - Transforms the input dataset with optional parameters.\n\nParameters:\t\n\ndataset – input dataset, which is an instance of pyspark.sql.DataFrame\n\nparams – an optional param map that overrides embedded params.\n\nReturns the transformed dataset"],"metadata":{}},{"cell_type":"code","source":["for row in result:\n    print(\"features=%s, label=%s -> prob=%s, prediction=%s\"\n          % (row.features, row.label, row.myProbability, row.prediction))"],"metadata":{},"outputs":[],"execution_count":27}],"metadata":{"name":"LogisticRegression_Estimator_Transformer_Param","notebookId":4270181377074593},"nbformat":4,"nbformat_minor":0}
