{"cells":[{"cell_type":"markdown","source":["# ML Pipelines with Saprk MLlib\n\nThis example covers the concepts of HashingTF, Tokenizer for text based pipeline.\n\nhttps://spark.apache.org/docs/latest/ml-pipeline.html"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## pyspark.ml.Pipeline\n\nA simple pipeline, which acts as an estimator. A Pipeline consists of a sequence of stages, each of which is either an Estimator or a Transformer. When Pipeline.fit() is called, the stages are executed in order. If a stage is an Estimator, its Estimator.fit() method will be called on the input dataset to fit a model. Then the model, which is a transformer, will be used to transform the dataset as the input to the next stage. If a stage is a Transformer, its Transformer.transform() method will be called to produce the dataset for the next stage. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages. If stages is an empty list, the pipeline acts as an identity transformer.\n\n<br>\n\n## pyspark.ml.feature.HashingTF\n \nMaps a sequence of terms to their term frequencies using the hashing trick. Currently we use Austin Appleby’s MurmurHash 3 algorithm (MurmurHash3_x86_32) to calculate the hash code value for the term object. Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a power of two as the numFeatures parameter; otherwise the features will not be mapped evenly to the columns.\n\n<br>\n\n## pyspark.ml.feature.RegexTokenizer\n\nA regex based tokenizer that extracts tokens either by using the provided regex pattern (in Java dialect) to split the text (default) or repeatedly matching the regex (if gaps is false). Optional parameters also allow filtering tokens using a minimal length. It returns an array of strings that can be empty."],"metadata":{}},{"cell_type":"code","source":["# Prepare training documents from a list of (id, text, label) tuples.\ndf_training = spark.createDataFrame([\n    (0, \"a b c d e spark\", 1.0),\n    (1, \"b d\", 0.0),\n    (2, \"spark f g h\", 1.0),\n    (3, \"hadoop mapreduce\", 0.0)\n], [\"id\", \"text\", \"label\"])"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\nlr = LogisticRegression(maxIter=10, regParam=0.001)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Tokenizer\n\nTokenizer is a unary transformer that converts the column of String values to lowercase and then splits it by white spaces."],"metadata":{}},{"cell_type":"code","source":["# Fit the pipeline to training documents.\nmodel = pipeline.fit(df_training)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Prepare test documents, which are unlabeled (id, text) tuples.\ndf_test = spark.createDataFrame([\n    (4, \"spark i j k\"),\n    (5, \"l m n\"),\n    (6, \"spark hadoop spark\"),\n    (7, \"apache hadoop\")\n], [\"id\", \"text\"])"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Make predictions on test documents and print columns of interest.\nprediction = model.transform(df_test)\nselected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\nfor row in selected.collect():\n    rid, text, prob, prediction = row\n    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Facts\n\nDAG Pipelines: A Pipeline’s stages are specified as an ordered array. The examples given here are all for linear Pipelines, i.e., Pipelines in which each stage uses data produced by the previous stage. It is possible to create non-linear Pipelines as long as the data flow graph forms a Directed Acyclic Graph (DAG). This graph is currently specified implicitly based on the input and output column names of each stage (generally specified as parameters). If the Pipeline forms a DAG, then the stages must be specified in topological order.\n\nRuntime checking: Since Pipelines can operate on DataFrames with varied types, they cannot use compile-time type checking. Pipelines and PipelineModels instead do runtime checking before actually running the Pipeline. This type checking is done using the DataFrame schema, a description of the data types of columns in the DataFrame.\n\nUnique Pipeline stages: A Pipeline’s stages should be unique instances. E.g., the same instance myHashingTF should not be inserted into the Pipeline twice since Pipeline stages must have unique IDs. However, different instances myHashingTF1 and myHashingTF2 (both of type HashingTF) can be put into the same Pipeline since different instances will be created with different IDs."],"metadata":{}}],"metadata":{"name":"LogisticRegression_HashingTF_Tokenizer_text_parsing_pipeline","notebookId":4270181377074611},"nbformat":4,"nbformat_minor":0}
